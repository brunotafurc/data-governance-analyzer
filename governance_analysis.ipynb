{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Governance Analysis\n",
        "This notebook runs all governance checks and saves results to a Delta table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create widgets for parameters\n",
        "dbutils.widgets.text(\"catalog_name\", \"main\", \"Catalog Name\")\n",
        "dbutils.widgets.text(\"schema_name\", \"default\", \"Schema Name\")\n",
        "\n",
        "# Get parameter values\n",
        "catalog_name = dbutils.widgets.get(\"catalog_name\")\n",
        "schema_name = dbutils.widgets.get(\"schema_name\")\n",
        "\n",
        "print(f\"Using catalog: {catalog_name}\")\n",
        "print(f\"Using schema: {schema_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import governance_analyzer as ga\n",
        "from datetime import datetime\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define governance checks\n",
        "checks = [\n",
        "    (\"Metastore setup\", \"Connect a Metastore to your Workspace\", ga.check_metastore_connected),\n",
        "    (\"Metastore setup\", \"The workspace is in the same region as the metastore\", ga.check_metastore_region),\n",
        "    (\"Identity\", \"Use SCIM or AIM from an Identity Provider\", ga.check_scim_aim_provisioning),\n",
        "    (\"Identity\", \"Account Admin role is assigned to a group\", ga.check_account_admin_group),\n",
        "    (\"Identity\", \"Metastore Admin role is assigned to a group\", ga.check_metastore_admin_group),\n",
        "    (\"Identity\", \"Workspace Admin role is assigned to a group\", ga.check_workspace_admin_group),\n",
        "    (\"Identity\", \"Catalog Admin role is assigned to a group\", ga.check_catalog_admin_group),\n",
        "    (\"Identity\", \"At least 1 user is an account admin\", ga.check_at_least_one_account_admin),\n",
        "    (\"Identity\", \"Less than 5% of users are Account Admin\", ga.check_account_admin_percentage),\n",
        "    (\"Managed Storage\", \"Create multiple Catalogs based on environment/BU/team\", ga.check_multiple_catalogs),\n",
        "    (\"Managed Storage\", \"No Catalog is bound to all workspaces\", ga.check_catalog_binding),\n",
        "    (\"Managed Storage\", \"Use Managed tables and volumes > 70%\", ga.check_managed_tables_percentage),\n",
        "    (\"Managed Storage\", \"No ADLS or S3 buckets outside UC\", ga.check_no_external_storage),\n",
        "    (\"Managed Storage\", \"No external volumes/tables at external location root\", ga.check_external_location_root),\n",
        "    (\"Managed Storage\", \"Independent storage credentials per external location\", ga.check_storage_credentials),\n",
        "    (\"Compute/Cluster Policy\", \"Compute is UC activated with right access mode\", ga.check_uc_compute),\n",
        "    (\"Migration Completeness\", \"No data in hive metastore\", ga.check_no_hive_data),\n",
        "    (\"Migration Completeness\", \"Hive metastore is disabled\", ga.check_hive_disabled),\n",
        "    (\"Migration Completeness\", \"0 mount storage accounts to DBFS\", ga.check_no_dbfs_mounts),\n",
        "    (\"Audit & Lineage Coverage\", \"All system tables activated (70%)\", ga.check_system_tables),\n",
        "    (\"Audit & Lineage Coverage\", \"70% of managed tables have predictive optimization\", ga.check_predictive_optimization),\n",
        "    (\"Audit & Lineage Coverage\", \"Data quality activated on 50% of tables\", ga.check_data_quality),\n",
        "    (\"Privileges\", \"Production jobs use service principals\", ga.check_service_principals),\n",
        "    (\"Privileges\", \"Modify access to production is limited\", ga.check_production_access),\n",
        "    (\"Privileges\", \"70% of assets have groups as owners\", ga.check_group_ownership),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run all checks\n",
        "results = []\n",
        "timestamp = datetime.now()\n",
        "\n",
        "for category, task_name, check_func in checks:\n",
        "    result = check_func()\n",
        "    results.append({\n",
        "        \"timestamp\": timestamp,\n",
        "        \"category\": category,\n",
        "        \"task_name\": task_name,\n",
        "        \"status\": result[\"status\"],\n",
        "        \"score\": result[\"score\"],\n",
        "        \"details\": result[\"details\"]\n",
        "    })\n",
        "\n",
        "print(f\"Completed {len(results)} governance checks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to DataFrame\n",
        "df = spark.createDataFrame(results)\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save to Delta table\n",
        "table_name = \"governance_results\"\n",
        "full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
        "\n",
        "df.write \\\n",
        "    .format(\"delta\") \\\n",
        "    .mode(\"append\") \\\n",
        "    .saveAsTable(full_table_name)\n",
        "\n",
        "print(f\"Results saved to {full_table_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate summary statistics\n",
        "summary_df = spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        category,\n",
        "        COUNT(*) as total_checks,\n",
        "        SUM(CASE WHEN status = 'pass' THEN 1 ELSE 0 END) as passed_checks,\n",
        "        SUM(score) as total_score,\n",
        "        ROUND(SUM(CASE WHEN status = 'pass' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as pass_rate\n",
        "    FROM {full_table_name}\n",
        "    WHERE timestamp = (SELECT MAX(timestamp) FROM {full_table_name})\n",
        "    GROUP BY category\n",
        "    ORDER BY category\n",
        "\"\"\")\n",
        "\n",
        "display(summary_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Overall governance score\n",
        "overall_df = spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        COUNT(*) as total_checks,\n",
        "        SUM(CASE WHEN status = 'pass' THEN 1 ELSE 0 END) as passed_checks,\n",
        "        SUM(score) as achieved_score,\n",
        "        ROUND(SUM(CASE WHEN status = 'pass' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as governance_score\n",
        "    FROM {full_table_name}\n",
        "    WHERE timestamp = (SELECT MAX(timestamp) FROM {full_table_name})\n",
        "\"\"\")\n",
        "\n",
        "display(overall_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dashboard Creation\n",
        "\n",
        "To create a dashboard in Databricks:\n",
        "\n",
        "1. Go to **SQL Workspace** in Databricks\n",
        "2. Create a new **SQL Query** for each visualization:\n",
        "   - **Governance Score by Category**: Use the summary query above\n",
        "   - **Overall Score**: Use the overall query above\n",
        "   - **Failed Checks**: Query for failed checks\n",
        "3. Create **Visualizations** for each query (bar charts, counters, tables)\n",
        "4. Create a new **Dashboard** and add all visualizations\n",
        "5. Set up **refresh schedule** if needed"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
