{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Governance Analysis\n",
        "This notebook runs all governance checks and saves results to a Delta table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create widgets for parameters\n",
        "dbutils.widgets.text(\"catalog_name\", \"main\", \"Catalog Name\")\n",
        "dbutils.widgets.text(\"schema_name\", \"default\", \"Schema Name\")\n",
        "\n",
        "# Get parameter values\n",
        "catalog_name = dbutils.widgets.get(\"catalog_name\")\n",
        "schema_name = dbutils.widgets.get(\"schema_name\")\n",
        "\n",
        "print(f\"Using catalog: {catalog_name}\")\n",
        "print(f\"Using schema: {schema_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import governance_analyzer as ga\n",
        "from datetime import datetime\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define governance checks\n",
        "checks = [\n",
        "    (\"Metastore setup\", \"Connect a Metastore to your Workspace\", ga.check_metastore_connected),\n",
        "    (\"Metastore setup\", \"The workspace is in the same region as the metastore\", ga.check_metastore_region),\n",
        "    (\"Identity\", \"Use SCIM or AIM from an Identity Provider\", ga.check_scim_aim_provisioning),\n",
        "    (\"Identity\", \"Account Admin role is assigned to a group\", ga.check_account_admin_group),\n",
        "    (\"Identity\", \"Metastore Admin role is assigned to a group\", ga.check_metastore_admin_group),\n",
        "    (\"Identity\", \"Workspace Admin role is assigned to a group\", ga.check_workspace_admin_group),\n",
        "    (\"Identity\", \"Catalog Admin role is assigned to a group\", ga.check_catalog_admin_group),\n",
        "    (\"Identity\", \"At least 1 user is an account admin\", ga.check_at_least_one_account_admin),\n",
        "    (\"Identity\", \"Less than 5% of users are Account Admin\", ga.check_account_admin_percentage),\n",
        "    (\"Managed Storage\", \"Create multiple Catalogs based on environment/BU/team\", ga.check_multiple_catalogs),\n",
        "    (\"Managed Storage\", \"No Catalog is bound to all workspaces\", ga.check_catalog_binding),\n",
        "    (\"Managed Storage\", \"Use Managed tables and volumes > 70%\", ga.check_managed_tables_percentage),\n",
        "    (\"Managed Storage\", \"No ADLS or S3 buckets outside UC\", ga.check_no_external_storage),\n",
        "    (\"Managed Storage\", \"No external volumes/tables at external location root\", ga.check_external_location_root),\n",
        "    (\"Managed Storage\", \"Independent storage credentials per external location\", ga.check_storage_credentials),\n",
        "    (\"Compute/Cluster Policy\", \"Compute is UC activated with right access mode\", ga.check_uc_compute),\n",
        "    (\"Migration Completeness\", \"No data in hive metastore\", ga.check_no_hive_data),\n",
        "    (\"Migration Completeness\", \"Hive metastore is disabled\", ga.check_hive_disabled),\n",
        "    (\"Migration Completeness\", \"0 mount storage accounts to DBFS\", ga.check_no_dbfs_mounts),\n",
        "    (\"Audit & Lineage Coverage\", \"All system tables activated (70%)\", ga.check_system_tables),\n",
        "    (\"Audit & Lineage Coverage\", \"70% of managed tables have predictive optimization\", ga.check_predictive_optimization),\n",
        "    (\"Audit & Lineage Coverage\", \"Data quality activated on 50% of tables\", ga.check_data_quality),\n",
        "    (\"Privileges\", \"Production jobs use service principals\", ga.check_service_principals),\n",
        "    (\"Privileges\", \"Modify access to production is limited\", ga.check_production_access),\n",
        "    (\"Privileges\", \"70% of assets have groups as owners\", ga.check_group_ownership),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run all checks\n",
        "results = []\n",
        "timestamp = datetime.now()\n",
        "\n",
        "for category, task_name, check_func in checks:\n",
        "    result = check_func()\n",
        "    max_score = result[\"max_score\"]\n",
        "    score = result[\"score\"]\n",
        "    score_percentage = round((score / max_score * 100.0) if max_score > 0 else 0.0, 2)\n",
        "    \n",
        "    results.append({\n",
        "        \"timestamp\": timestamp,\n",
        "        \"category\": category,\n",
        "        \"task_name\": task_name,\n",
        "        \"status\": result[\"status\"],\n",
        "        \"score\": score,\n",
        "        \"max_score\": max_score,\n",
        "        \"score_percentage\": float(score_percentage),\n",
        "        \"details\": result[\"details\"]\n",
        "    })\n",
        "\n",
        "print(f\"Completed {len(results)} governance checks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to DataFrame\n",
        "df = spark.createDataFrame(results)\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save to Delta table\n",
        "table_name = \"governance_results\"\n",
        "full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
        "\n",
        "df.write \\\n",
        "    .format(\"delta\") \\\n",
        "    .mode(\"append\") \\\n",
        "    .option(\"mergeSchema\", \"true\") \\\n",
        "    .saveAsTable(full_table_name)\n",
        "\n",
        "print(f\"✓ Results saved to {full_table_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Deploy Lakeview Dashboard\n",
        "print(\"Creating Lakeview Dashboard...\")\n",
        "\n",
        "try:\n",
        "    dashboard_result = ga.create_dashboard(\n",
        "        catalog_name=catalog_name,\n",
        "        schema_name=schema_name,\n",
        "        folder_path=\"/Shared/Governance\",\n",
        "        dashboard_name=f\"Governance Results Dashboard - {catalog_name}.{schema_name}\"\n",
        "    )\n",
        "    \n",
        "    if dashboard_result[\"status\"] == \"success\":\n",
        "        print(f\"✓ {dashboard_result['message']}\")\n",
        "        print(f\"  Dashboard URL: {dashboard_result['workspace_url']}\")\n",
        "        print(f\"  Dashboard Path: {dashboard_result['dashboard_path']}\")\n",
        "    else:\n",
        "        print(f\"⚠ Dashboard creation failed: {dashboard_result['message']}\")\n",
        "        print(f\"  Note: You can manually import the dashboard from dashboard_template.lvdash.json\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠ Could not create dashboard: {str(e)}\")\n",
        "    print(\"  Note: You can manually import the dashboard from dashboard_template.lvdash.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate summary statistics\n",
        "summary_df = spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        category,\n",
        "        COUNT(*) as total_checks,\n",
        "        SUM(CASE WHEN status = 'pass' THEN 1 ELSE 0 END) as passed_checks,\n",
        "        SUM(score) as achieved_score,\n",
        "        SUM(max_score) as total_possible_score,\n",
        "        ROUND(SUM(score) * 100.0 / NULLIF(SUM(max_score), 0), 2) as score_percentage,\n",
        "        ROUND(SUM(CASE WHEN status = 'pass' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as pass_rate\n",
        "    FROM {full_table_name}\n",
        "    WHERE timestamp = (SELECT MAX(timestamp) FROM {full_table_name})\n",
        "    GROUP BY category\n",
        "    ORDER BY category\n",
        "\"\"\")\n",
        "\n",
        "display(summary_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Overall governance score\n",
        "overall_df = spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        COUNT(*) as total_checks,\n",
        "        SUM(CASE WHEN status = 'pass' THEN 1 ELSE 0 END) as passed_checks,\n",
        "        SUM(score) as achieved_score,\n",
        "        SUM(max_score) as total_possible_score,\n",
        "        ROUND(SUM(score) * 100.0 / NULLIF(SUM(max_score), 0), 2) as score_percentage,\n",
        "        ROUND(SUM(CASE WHEN status = 'pass' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as pass_rate\n",
        "    FROM {full_table_name}\n",
        "    WHERE timestamp = (SELECT MAX(timestamp) FROM {full_table_name})\n",
        "\"\"\")\n",
        "\n",
        "display(overall_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dashboard\n",
        "\n",
        "A Lakeview Dashboard has been automatically created at `/Shared/Governance/` with the following visualizations:\n",
        "\n",
        "- **Total Checks Counter**: Shows the total number of governance checks performed\n",
        "- **Total Score Counter**: Displays the aggregated governance score\n",
        "- **Pass Rate Counter**: Shows the percentage of checks that passed\n",
        "- **Average Score by Category**: Bar chart showing performance across categories\n",
        "- **Status Distribution**: Pie chart showing pass/fail distribution\n",
        "- **Detailed Results Table**: Full table with all governance check results\n",
        "\n",
        "### Manual Import (if automatic creation failed)\n",
        "\n",
        "If the dashboard wasn't created automatically, you can manually import it:\n",
        "\n",
        "1. Go to **SQL Workspace** in Databricks\n",
        "2. Click **Dashboards** → **Create Dashboard** → **Import Dashboard**\n",
        "3. Upload the `dashboard_template.lvdash.json` file from this project\n",
        "4. Update the dataset query to point to your table: `{catalog}.{schema}.governance_results`"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
