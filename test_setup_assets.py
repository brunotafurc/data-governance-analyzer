# Databricks notebook source
# MAGIC %md
# MAGIC # Governance Checks â€” Test Asset Setup
# MAGIC
# MAGIC This notebook creates **all** the infrastructure needed to test the 5 governance checks.
# MAGIC
# MAGIC ### What YOU need before running:
# MAGIC 1. An **ADLS Gen 2** storage account + container
# MAGIC 2. An **Azure Service Principal** (App Registration) with `Storage Blob Data Contributor` role on the storage account
# MAGIC 3. The service principal's **Client ID**, **Client Secret**, and **Tenant ID**
# MAGIC
# MAGIC ### What this notebook creates:
# MAGIC | Asset | Purpose | Check it tests |
# MAGIC |-------|---------|----------------|
# MAGIC | 2 catalogs (PO on/off) + schemas + 10 managed tables | Predictive Optimization | `check_predictive_optimization` |
# MAGIC | 1 Storage Credential | Access ADLS from UC | `check_storage_credentials` |
# MAGIC | 2 External Locations (same credential) | Shared credential = FAIL | `check_storage_credentials` |
# MAGIC | 1 External table at location root | Object at root = FAIL | `check_external_location_root` |
# MAGIC | 1 DBFS mount | Legacy mount = FAIL | `check_no_dbfs_mounts` |
# MAGIC | Lakehouse Monitors on some tables | < 50% monitored = FAIL | `check_data_quality` |
# MAGIC
# MAGIC ### Expected results:
# MAGIC | Check | Expected | Why |
# MAGIC |-------|----------|-----|
# MAGIC | Predictive Optimization | âœ… PASS (70%) | 7/10 tables in PO-enabled catalog |
# MAGIC | DBFS Mounts | âŒ FAIL | We create 1 mount |
# MAGIC | External Location Root | âŒ FAIL | We place a table at the root |
# MAGIC | Storage Credentials | âŒ FAIL | 2 locations share 1 credential |
# MAGIC | Data Quality | âŒ FAIL (40%) | Only 4/10 tables monitored |

# COMMAND ----------

%pip install databricks-sdk --upgrade --quiet
dbutils.library.restartPython()

# COMMAND ----------

# MAGIC %md
# MAGIC ## Parameters
# MAGIC
# MAGIC Fill in the widgets. The **prefix** is always required.
# MAGIC
# MAGIC - For **core tests** (Predictive Optimization only): just run with the prefix.
# MAGIC - For **all tests**: fill in the Azure fields.

# COMMAND ----------

# -- Widgets ----------------------------------------------------------------
dbutils.widgets.text("prefix", "gov_test", "1. Resource prefix")
dbutils.widgets.text("adls_container_url", "", "2. ADLS URL: abfss://container@account.dfs.core.windows.net")
dbutils.widgets.text("sp_client_id", "", "3. Service Principal Client ID")
dbutils.widgets.text("sp_client_secret", "", "4. Service Principal Client Secret")
dbutils.widgets.text("sp_tenant_id", "", "5. Service Principal Tenant ID")
dbutils.widgets.text("warehouse_id", "", "6. SQL Warehouse ID (for monitors, optional)")

# -- Read parameters --------------------------------------------------------
PREFIX = dbutils.widgets.get("prefix").strip()
ADLS_URL = dbutils.widgets.get("adls_container_url").strip().rstrip("/") or None
SP_CLIENT_ID = dbutils.widgets.get("sp_client_id").strip() or None
SP_CLIENT_SECRET = dbutils.widgets.get("sp_client_secret").strip() or None
SP_TENANT_ID = dbutils.widgets.get("sp_tenant_id").strip() or None
WAREHOUSE_ID = dbutils.widgets.get("warehouse_id").strip() or None

AZURE_CONFIGURED = all([ADLS_URL, SP_CLIENT_ID, SP_CLIENT_SECRET, SP_TENANT_ID])

print(f"Prefix:           {PREFIX}")
print(f"ADLS URL:         {ADLS_URL or '(not set)'}")
print(f"Service Principal: {'configured âœ“' if AZURE_CONFIGURED else '(incomplete â€” external location + mount tests will be skipped)'}")
print(f"Warehouse ID:     {WAREHOUSE_ID or '(not set â€” data quality monitor tests will be skipped)'}")

if ADLS_URL and not AZURE_CONFIGURED:
    print("\nâš ï¸  You provided an ADLS URL but not all Service Principal fields.")
    print("   Fill in sp_client_id, sp_client_secret, and sp_tenant_id to enable all tests.")

# COMMAND ----------

# MAGIC %md
# MAGIC ---
# MAGIC ## 1. Core Setup â€” Catalogs, Schemas, Managed Tables, Predictive Optimization
# MAGIC
# MAGIC **No cloud infrastructure needed.** This section creates Unity Catalog objects
# MAGIC to test `check_predictive_optimization`.

# COMMAND ----------

from databricks.sdk import WorkspaceClient
from databricks.sdk.service.catalog import EnablePredictiveOptimization

w = WorkspaceClient()

# -- Naming conventions -----------------------------------------------------
CAT_PO_ON = f"{PREFIX}_po_on"
CAT_PO_OFF = f"{PREFIX}_po_off"
SCHEMA_NAME = "test_data"

# -- Create catalogs --------------------------------------------------------
for cat_name in [CAT_PO_ON, CAT_PO_OFF]:
    try:
        w.catalogs.create(name=cat_name)
        print(f"âœ“ Created catalog: {cat_name}")
    except Exception as e:
        if "already exists" in str(e).lower():
            print(f"â€¢ Catalog already exists: {cat_name}")
        else:
            raise

# -- Configure Predictive Optimization -------------------------------------
w.catalogs.update(
    name=CAT_PO_ON,
    enable_predictive_optimization=EnablePredictiveOptimization.ENABLE,
)
print(f"âœ“ Predictive Optimization ENABLED on {CAT_PO_ON}")

w.catalogs.update(
    name=CAT_PO_OFF,
    enable_predictive_optimization=EnablePredictiveOptimization.DISABLE,
)
print(f"âœ“ Predictive Optimization DISABLED on {CAT_PO_OFF}")

# -- Create schemas ---------------------------------------------------------
for cat_name in [CAT_PO_ON, CAT_PO_OFF]:
    try:
        w.schemas.create(name=SCHEMA_NAME, catalog_name=cat_name)
        print(f"âœ“ Created schema: {cat_name}.{SCHEMA_NAME}")
    except Exception as e:
        if "already exists" in str(e).lower():
            print(f"â€¢ Schema already exists: {cat_name}.{SCHEMA_NAME}")
        else:
            raise

# COMMAND ----------

# -- Create managed tables ---------------------------------------------------
# 7 tables in PO-ON catalog + 3 in PO-OFF catalog = 70% with PO (threshold)

PO_ON_TABLES = [f"managed_tbl_{i}" for i in range(1, 8)]   # 7 tables
PO_OFF_TABLES = [f"managed_tbl_{i}" for i in range(1, 4)]  # 3 tables

for cat_name, table_list in [(CAT_PO_ON, PO_ON_TABLES), (CAT_PO_OFF, PO_OFF_TABLES)]:
    for tbl in table_list:
        full_name = f"`{cat_name}`.`{SCHEMA_NAME}`.`{tbl}`"
        try:
            spark.sql(f"""
                CREATE TABLE IF NOT EXISTS {full_name} (
                    id BIGINT,
                    name STRING,
                    value DOUBLE,
                    created_at TIMESTAMP
                )
            """)
            spark.sql(f"""
                INSERT INTO {full_name} VALUES
                (1, 'alpha', 10.5, current_timestamp()),
                (2, 'beta',  20.3, current_timestamp()),
                (3, 'gamma', 30.1, current_timestamp())
            """)
            print(f"âœ“ Created + populated: {cat_name}.{SCHEMA_NAME}.{tbl}")
        except Exception as e:
            print(f"âœ— Error creating {full_name}: {e}")

# COMMAND ----------

total = len(PO_ON_TABLES) + len(PO_OFF_TABLES)
pct = round(len(PO_ON_TABLES) / total * 100, 1)

print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  âœ“ CORE SETUP COMPLETE                                         â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Catalogs: {CAT_PO_ON} (PO=ON), {CAT_PO_OFF} (PO=OFF)
â•‘  Schema:   {SCHEMA_NAME}
â•‘  Tables:   {len(PO_ON_TABLES)} in PO-ON + {len(PO_OFF_TABLES)} in PO-OFF = {total}
â•‘  Expected: check_predictive_optimization â†’ PASS ({pct}% â‰¥ 70%)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

# COMMAND ----------

# MAGIC %md
# MAGIC ---
# MAGIC ## 2. Storage Credential + External Locations
# MAGIC
# MAGIC **Requires:** ADLS URL + Service Principal credentials.
# MAGIC
# MAGIC This section creates:
# MAGIC 1. A **Storage Credential** in Unity Catalog using your Azure Service Principal
# MAGIC 2. Two **External Locations** pointing to different paths but using the **same** credential
# MAGIC 3. An **External Table at the root** of one external location
# MAGIC
# MAGIC This makes `check_storage_credentials` **FAIL** (shared credential)
# MAGIC and `check_external_location_root` **FAIL** (table at root).

# COMMAND ----------

if AZURE_CONFIGURED:
    from databricks.sdk.service.catalog import AzureServicePrincipal

    CRED_NAME = f"{PREFIX}_credential"
    EXT_LOC_1 = f"{PREFIX}_ext_loc_1"
    EXT_LOC_2 = f"{PREFIX}_ext_loc_2"
    EXT_URL_1 = f"{ADLS_URL}/{PREFIX}_loc1"
    EXT_URL_2 = f"{ADLS_URL}/{PREFIX}_loc2"

    # -- Step 1: Create Storage Credential -----------------------------------
    print("â”€â”€ Creating Storage Credential â”€â”€")
    try:
        w.storage_credentials.create(
            name=CRED_NAME,
            azure_service_principal=AzureServicePrincipal(
                directory_id=SP_TENANT_ID,
                application_id=SP_CLIENT_ID,
                client_secret=SP_CLIENT_SECRET,
            ),
            comment=f"Test credential for governance checks (prefix: {PREFIX})",
        )
        print(f"âœ“ Created storage credential: {CRED_NAME}")
    except Exception as e:
        if "already exists" in str(e).lower():
            print(f"â€¢ Storage credential already exists: {CRED_NAME}")
        else:
            print(f"âœ— Error creating storage credential: {e}")
            print("  Make sure your Service Principal has 'Storage Blob Data Contributor' role")
            print("  on the ADLS Gen 2 storage account.")
            raise

    # -- Step 2: Create 2 External Locations with SAME credential (fail case) -
    print("\nâ”€â”€ Creating External Locations (shared credential â†’ FAIL case) â”€â”€")
    for loc_name, loc_url in [(EXT_LOC_1, EXT_URL_1), (EXT_LOC_2, EXT_URL_2)]:
        try:
            w.external_locations.create(
                name=loc_name,
                url=loc_url,
                credential_name=CRED_NAME,
                skip_validation=True,
                comment=f"Test location for governance checks",
            )
            print(f"âœ“ Created external location: {loc_name}")
            print(f"  URL: {loc_url}")
            print(f"  Credential: {CRED_NAME}")
        except Exception as e:
            if "already exists" in str(e).lower():
                print(f"â€¢ External location already exists: {loc_name}")
            else:
                print(f"âœ— Error creating {loc_name}: {e}")

    # -- Step 3: Create external table AT root of ext location 1 (fail case) -
    print("\nâ”€â”€ Creating External Table at Location Root (â†’ FAIL case) â”€â”€")
    EXT_SCHEMA = f"{PREFIX}_ext_schema"
    try:
        w.schemas.create(name=EXT_SCHEMA, catalog_name=CAT_PO_OFF)
        print(f"âœ“ Created schema: {CAT_PO_OFF}.{EXT_SCHEMA}")
    except Exception as e:
        if "already exists" in str(e).lower():
            print(f"â€¢ Schema already exists: {CAT_PO_OFF}.{EXT_SCHEMA}")
        else:
            raise

    ext_table_at_root = f"`{CAT_PO_OFF}`.`{EXT_SCHEMA}`.`table_at_root`"
    try:
        spark.sql(f"""
            CREATE TABLE IF NOT EXISTS {ext_table_at_root} (
                id BIGINT, data STRING
            )
            USING DELTA
            LOCATION '{EXT_URL_1}'
        """)
        print(f"âœ“ Created EXTERNAL table at location root: {ext_table_at_root}")
        print(f"  Location: {EXT_URL_1} (= root of {EXT_LOC_1})")
    except Exception as e:
        print(f"âœ— Error creating external table at root: {e}")

    # Also create one in a subdirectory (good practice - not a violation)
    ext_table_subdir = f"`{CAT_PO_OFF}`.`{EXT_SCHEMA}`.`table_in_subdir`"
    try:
        spark.sql(f"""
            CREATE TABLE IF NOT EXISTS {ext_table_subdir} (
                id BIGINT, data STRING
            )
            USING DELTA
            LOCATION '{EXT_URL_2}/subdir/my_table'
        """)
        print(f"âœ“ Created EXTERNAL table in subdirectory: {ext_table_subdir}")
        print(f"  Location: {EXT_URL_2}/subdir/my_table (= subdir of {EXT_LOC_2} âœ“)")
    except Exception as e:
        print(f"âœ— Error creating external table in subdir: {e}")

    print(f"""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  âœ“ EXTERNAL LOCATIONS SETUP COMPLETE                          â”‚
    â”‚                                                                â”‚
    â”‚  Storage Credential: {CRED_NAME}
    â”‚  External Location 1: {EXT_LOC_1} â†’ {EXT_URL_1}
    â”‚  External Location 2: {EXT_LOC_2} â†’ {EXT_URL_2}
    â”‚                                                                â”‚
    â”‚  Expected: check_storage_credentials    â†’ âŒ FAIL              â”‚
    â”‚            (both locations share '{CRED_NAME}')
    â”‚  Expected: check_external_location_root â†’ âŒ FAIL              â”‚
    â”‚            (table_at_root is at ext location root)             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
else:
    print("â­  Skipping external locations setup")
    print("   To enable: fill in adls_container_url, sp_client_id, sp_client_secret, sp_tenant_id")

# COMMAND ----------

# MAGIC %md
# MAGIC ---
# MAGIC ## 3. DBFS Mount
# MAGIC
# MAGIC **Requires:** ADLS URL + Service Principal credentials.
# MAGIC
# MAGIC Creates a legacy DBFS mount using your Service Principal's OAuth credentials.
# MAGIC This makes `check_no_dbfs_mounts` **FAIL**.

# COMMAND ----------

if AZURE_CONFIGURED:
    MOUNT_POINT = f"/mnt/{PREFIX}_test_mount"

    # Parse storage account name from ADLS URL
    # Expected format: abfss://container@account.dfs.core.windows.net
    import re
    adls_match = re.match(r"abfss://([^@]+)@([^.]+)\.dfs\.core\.windows\.net(.*)", ADLS_URL)

    if adls_match:
        container = adls_match.group(1)
        storage_account = adls_match.group(2)
        path_suffix = adls_match.group(3) or ""

        # Check if already mounted
        existing_mounts = {m.mountPoint for m in dbutils.fs.mounts()}

        if MOUNT_POINT in existing_mounts:
            print(f"â€¢ Mount already exists: {MOUNT_POINT}")
        else:
            print(f"â”€â”€ Creating DBFS Mount â”€â”€")
            print(f"  Mount point: {MOUNT_POINT}")
            print(f"  Source:      {ADLS_URL}")
            print(f"  Auth:        OAuth via Service Principal")

            try:
                configs = {
                    "fs.azure.account.auth.type": "OAuth",
                    "fs.azure.account.oauth.provider.type": "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider",
                    "fs.azure.account.oauth2.client.id": SP_CLIENT_ID,
                    "fs.azure.account.oauth2.client.secret": SP_CLIENT_SECRET,
                    "fs.azure.account.oauth2.client.endpoint": f"https://login.microsoftonline.com/{SP_TENANT_ID}/oauth2/token",
                }

                dbutils.fs.mount(
                    source=ADLS_URL,
                    mount_point=MOUNT_POINT,
                    extra_configs=configs,
                )
                print(f"âœ“ Created DBFS mount: {MOUNT_POINT}")
            except Exception as e:
                print(f"âœ— Could not create DBFS mount: {e}")
                print(f"  Common issues:")
                print(f"  - Service Principal doesn't have 'Storage Blob Data Contributor' on the storage account")
                print(f"  - Firewall/network rules on the storage account blocking access")

        print(f"""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  âœ“ DBFS MOUNT SETUP COMPLETE                         â”‚
    â”‚  Mount: {MOUNT_POINT} â†’ {ADLS_URL}
    â”‚  Expected: check_no_dbfs_mounts â†’ âŒ FAIL             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """)
    else:
        print(f"âœ— Could not parse ADLS URL: {ADLS_URL}")
        print(f"  Expected format: abfss://container@account.dfs.core.windows.net")
        print(f"  Example:         abfss://data@mystorageaccount.dfs.core.windows.net")
else:
    print("â­  Skipping DBFS mount setup")
    print("   To enable: fill in adls_container_url, sp_client_id, sp_client_secret, sp_tenant_id")

# COMMAND ----------

# MAGIC %md
# MAGIC ---
# MAGIC ## 4. Data Quality Monitors (optional)
# MAGIC
# MAGIC **Requires:** `warehouse_id` widget (a running SQL Warehouse).
# MAGIC
# MAGIC Creates Lakehouse Monitors on 4 out of 10 tables = 40% â†’ below 50% threshold â†’ **FAIL**.

# COMMAND ----------

if WAREHOUSE_ID:
    MONITOR_OUTPUT_SCHEMA = f"{CAT_PO_ON}.{SCHEMA_NAME}"
    MONITOR_ASSETS_DIR = f"/Shared/{PREFIX}_monitor_assets"
    tables_to_monitor = PO_ON_TABLES[:4]  # Monitor 4 of 7 tables in PO-ON catalog

    print(f"â”€â”€ Creating Lakehouse Monitors â”€â”€")
    print(f"  Warehouse: {WAREHOUSE_ID}")
    print(f"  Output:    {MONITOR_OUTPUT_SCHEMA}")
    print(f"  Assets:    {MONITOR_ASSETS_DIR}")
    print()

    created = 0
    for tbl in tables_to_monitor:
        full_name = f"{CAT_PO_ON}.{SCHEMA_NAME}.{tbl}"
        try:
            w.lakehouse_monitors.create(
                table_name=full_name,
                assets_dir=MONITOR_ASSETS_DIR,
                output_schema_name=MONITOR_OUTPUT_SCHEMA,
                snapshot={},  # Snapshot-based monitoring (simplest)
                warehouse_id=WAREHOUSE_ID,
            )
            print(f"  âœ“ Monitor created: {full_name}")
            created += 1
        except Exception as e:
            if "already" in str(e).lower() or "exists" in str(e).lower():
                print(f"  â€¢ Monitor already exists: {full_name}")
                created += 1
            else:
                print(f"  âœ— Error creating monitor for {full_name}: {e}")

    total_count = len(PO_ON_TABLES) + len(PO_OFF_TABLES)
    pct = round(created / total_count * 100, 1) if total_count > 0 else 0
    print(f"""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  âœ“ DATA QUALITY MONITORS SETUP COMPLETE                 â”‚
    â”‚  Monitored: {created}/{total_count} tables = {pct}%
    â”‚  Expected: check_data_quality â†’ âŒ FAIL ({pct}% < 50%)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
else:
    print("â­  Skipping data quality monitors")
    print("   To enable: fill in warehouse_id (ID of a running SQL Warehouse)")
    print("   Find it in: SQL Warehouses â†’ your warehouse â†’ Connection Details â†’ HTTP Path â†’ last segment")

# COMMAND ----------

# MAGIC %md
# MAGIC ---
# MAGIC ## 5. Run All 5 Governance Checks
# MAGIC
# MAGIC This is the moment of truth â€” let's see if our checks detect the assets we created.

# COMMAND ----------

import governance_analyzer as ga

checks_to_test = [
    ("Predictive Optimization", "â‰¥70% managed tables with PO",  ga.check_predictive_optimization),
    ("No DBFS Mounts",          "0 legacy DBFS mounts",         ga.check_no_dbfs_mounts),
    ("External Location Root",  "No objects at ext loc root",   ga.check_external_location_root),
    ("Storage Credentials",     "Independent creds per ext loc", ga.check_storage_credentials),
    ("Data Quality",            "â‰¥50% tables with monitoring",  ga.check_data_quality),
]

print("=" * 80)
print("  GOVERNANCE CHECK RESULTS")
print("=" * 80)

for name, description, fn in checks_to_test:
    result = fn()
    status = result["status"].upper()
    icon = {"PASS": "âœ…", "FAIL": "âŒ", "WARNING": "âš ï¸ ", "ERROR": "ğŸ”´"}.get(status, "â“")

    print(f"\n{icon} [{status:7s}]  {name}")
    print(f"   Rule:    {description}")
    print(f"   Score:   {result['score']}/{result['max_score']}")
    print(f"   Details: {result['details']}")

print("\n" + "=" * 80)

# COMMAND ----------

# MAGIC %md
# MAGIC ---
# MAGIC ## 6. Cleanup â€” Remove All Test Assets
# MAGIC
# MAGIC **âš ï¸  Uncomment the code below and run it to delete everything this notebook created.**

# COMMAND ----------

# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚  UNCOMMENT THIS ENTIRE BLOCK TO CLEAN UP ALL TEST ASSETS              â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

# from databricks.sdk import WorkspaceClient
# w = WorkspaceClient()

# PREFIX = dbutils.widgets.get("prefix")
# CAT_PO_ON = f"{PREFIX}_po_on"
# CAT_PO_OFF = f"{PREFIX}_po_off"
# SCHEMA_NAME = "test_data"
# PO_ON_TABLES = [f"managed_tbl_{i}" for i in range(1, 8)]
# CRED_NAME = f"{PREFIX}_credential"

# print("â”€â”€ Cleaning up test assets â”€â”€\n")

# # 1. Delete Lakehouse Monitors
# for tbl in PO_ON_TABLES[:4]:
#     full_name = f"{CAT_PO_ON}.{SCHEMA_NAME}.{tbl}"
#     try:
#         w.lakehouse_monitors.delete(table_name=full_name)
#         print(f"  âœ“ Deleted monitor: {full_name}")
#     except Exception:
#         pass

# # 2. Delete external locations
# for loc_name in [f"{PREFIX}_ext_loc_1", f"{PREFIX}_ext_loc_2"]:
#     try:
#         w.external_locations.delete(name=loc_name, force=True)
#         print(f"  âœ“ Deleted external location: {loc_name}")
#     except Exception:
#         pass

# # 3. Delete storage credential
# try:
#     w.storage_credentials.delete(name=CRED_NAME, force=True)
#     print(f"  âœ“ Deleted storage credential: {CRED_NAME}")
# except Exception:
#     pass

# # 4. Unmount DBFS
# try:
#     dbutils.fs.unmount(f"/mnt/{PREFIX}_test_mount")
#     print(f"  âœ“ Unmounted: /mnt/{PREFIX}_test_mount")
# except Exception:
#     pass

# # 5. Drop catalogs (CASCADE drops all schemas, tables, volumes inside)
# for cat_name in [CAT_PO_ON, CAT_PO_OFF]:
#     try:
#         spark.sql(f"DROP CATALOG IF EXISTS `{cat_name}` CASCADE")
#         print(f"  âœ“ Dropped catalog: {cat_name}")
#     except Exception as e:
#         print(f"  âœ— Error dropping {cat_name}: {e}")

# print("\nâœ“ All test assets cleaned up!")
